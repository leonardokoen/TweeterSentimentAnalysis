{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a62d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Stack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Stack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Stack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import *\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "import preprocessor as p\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbff6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('clean_train_data_big.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7c8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_tweets.csv')\n",
    "df_train.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22351633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@TBthatsme what!!!! Why didn't you tell us? Uh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@stuartsharpe Ah you're saying being obsessed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@lejjewellery is it your birthday today? :o ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@TerrenceJ106 Hey babe follow me,love u on da ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>working hard as always on TNARevolution.com, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              tweet\n",
       "0     1.0  @TBthatsme what!!!! Why didn't you tell us? Uh...\n",
       "1     1.0  @stuartsharpe Ah you're saying being obsessed ...\n",
       "2     0.0  @lejjewellery is it your birthday today? :o ye...\n",
       "3     1.0  @TerrenceJ106 Hey babe follow me,love u on da ...\n",
       "4     0.0  working hard as always on TNARevolution.com, t..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05850287",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "  #Removes Numbers\n",
    "    stemmer = PorterStemmer()\n",
    "    data = data.astype(str).str.replace('\\d+', '')\n",
    "    lower_text = data.str.lower()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    w_tokenizer =  TweetTokenizer()\n",
    "    \n",
    "    def stemming(text):\n",
    "        return [stemmer.stem(w) for w in text]\n",
    "    \n",
    "    def lemmatize_text(text):\n",
    "        return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "    \n",
    "    def remove_punctuation(words):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    words = lower_text.apply(lemmatize_text)\n",
    "    words = words.apply(remove_punctuation)\n",
    "    words = words.apply(stemming)\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(words)\n",
    "\n",
    "def preprocess(df):\n",
    "    \n",
    "    df['hashtag'] = df['tweet'].apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "    df['text'] = 0\n",
    "    for i in range(0,len(df)):\n",
    "        df['text'][i] = p.clean(df['tweet'][i])\n",
    "        \n",
    "    df['text_helper'] = df['text'].copy()\n",
    "    df['text_helper'] = preprocess_data(df['text_helper']) \n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['text_helper'] = df['text_helper'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    a = len(max(np.array(df_train['text_helper']), key=len))\n",
    "    \n",
    "    \n",
    "    def convert_to_string(listWord):\n",
    "        return ' '.join(listWord)\n",
    "    df['text_helper'] = df['text_helper'].apply(convert_to_string)\n",
    "   \n",
    "    return a\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e8a1f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stack\\AppData\\Local\\Temp\\ipykernel_856\\3072535002.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i] = p.clean(df['tweet'][i])\n",
      "C:\\Users\\Stack\\AppData\\Local\\Temp\\ipykernel_856\\3072535002.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.astype(str).str.replace('\\d+', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "max_tweet_word_count = preprocess(df_train)\n",
    "\n",
    "print(max_tweet_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d60a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"preprocessed_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcac0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('preprocessed_tweets.csv')\n",
    "max_tweet_word_count = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0038d95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>text</th>\n",
       "      <th>text_helper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@TBthatsme what!!!! Why didn't you tell us? Uh...</td>\n",
       "      <td>[]</td>\n",
       "      <td>what!!!! Why didn't you tell us? Uhhh... You s...</td>\n",
       "      <td>whi didnt tell u uhhh suck haha im kid got hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@stuartsharpe Ah you're saying being obsessed ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ah you're saying being obsessed with his body ...</td>\n",
       "      <td>ah say obsess hi bodi languag onli thing keep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@lejjewellery is it your birthday today? :o ye...</td>\n",
       "      <td>[]</td>\n",
       "      <td>is it your birthday today? yeah, im worries no...</td>\n",
       "      <td>birthday today yeah im worri im go get humunga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@TerrenceJ106 Hey babe follow me,love u on da ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hey babe follow me,love u on da show</td>\n",
       "      <td>hey babe follow love u da show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>working hard as always on TNARevolution.com, t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>working hard as always on TNARevolution.com, t...</td>\n",
       "      <td>work hard alway tnarevolutioncom think time sl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              tweet hashtag  \\\n",
       "0     1.0  @TBthatsme what!!!! Why didn't you tell us? Uh...      []   \n",
       "1     1.0  @stuartsharpe Ah you're saying being obsessed ...      []   \n",
       "2     0.0  @lejjewellery is it your birthday today? :o ye...      []   \n",
       "3     1.0  @TerrenceJ106 Hey babe follow me,love u on da ...      []   \n",
       "4     0.0  working hard as always on TNARevolution.com, t...      []   \n",
       "\n",
       "                                                text  \\\n",
       "0  what!!!! Why didn't you tell us? Uhhh... You s...   \n",
       "1  Ah you're saying being obsessed with his body ...   \n",
       "2  is it your birthday today? yeah, im worries no...   \n",
       "3               Hey babe follow me,love u on da show   \n",
       "4  working hard as always on TNARevolution.com, t...   \n",
       "\n",
       "                                         text_helper  \n",
       "0  whi didnt tell u uhhh suck haha im kid got hom...  \n",
       "1  ah say obsess hi bodi languag onli thing keep ...  \n",
       "2  birthday today yeah im worri im go get humunga...  \n",
       "3                     hey babe follow love u da show  \n",
       "4  work hard alway tnarevolutioncom think time sl...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce491d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "empty_indexes = []\n",
    "for i in range(0,len(df_train['text_helper'])):\n",
    "    try:\n",
    "        list_of_words.extend(df_train['text_helper'][i].split())\n",
    "    except:\n",
    "        empty_indexes.append(i)\n",
    "    \n",
    "total_words = len(list(set(list_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98c100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(empty_indexes)\n",
    "df_train = df_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83d104db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>text</th>\n",
       "      <th>text_helper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@TBthatsme what!!!! Why didn't you tell us? Uh...</td>\n",
       "      <td>[]</td>\n",
       "      <td>what!!!! Why didn't you tell us? Uhhh... You s...</td>\n",
       "      <td>whi didnt tell u uhhh suck haha im kid got hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@stuartsharpe Ah you're saying being obsessed ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ah you're saying being obsessed with his body ...</td>\n",
       "      <td>ah say obsess hi bodi languag onli thing keep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@lejjewellery is it your birthday today? :o ye...</td>\n",
       "      <td>[]</td>\n",
       "      <td>is it your birthday today? yeah, im worries no...</td>\n",
       "      <td>birthday today yeah im worri im go get humunga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@TerrenceJ106 Hey babe follow me,love u on da ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hey babe follow me,love u on da show</td>\n",
       "      <td>hey babe follow love u da show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>working hard as always on TNARevolution.com, t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>working hard as always on TNARevolution.com, t...</td>\n",
       "      <td>work hard alway tnarevolutioncom think time sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592339</th>\n",
       "      <td>1599994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@ScruffyPanther  when's the last exam mdear? X</td>\n",
       "      <td>[]</td>\n",
       "      <td>when's the last exam mdear? X</td>\n",
       "      <td>last exam mdear x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592340</th>\n",
       "      <td>1599995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Aaa lunch time. So loaded w work  but its ok. ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Aaa lunch time. So loaded w work but its ok. M...</td>\n",
       "      <td>aaa lunch time load w work ok mani peopl arent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592341</th>\n",
       "      <td>1599996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'm watching the movie ''The curious case of B...</td>\n",
       "      <td>[]</td>\n",
       "      <td>I'm watching the movie ''The curious case of B...</td>\n",
       "      <td>im watch movi curiou case benjamin button good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592342</th>\n",
       "      <td>1599997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the scene where amanda peets walks into the gl...</td>\n",
       "      <td>[]</td>\n",
       "      <td>the scene where amanda peets walks into the gl...</td>\n",
       "      <td>scene amanda peet walk glass door lot like lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592343</th>\n",
       "      <td>1599998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@blackquestions yes, but now i know how YOU ar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>yes, but now i know how YOU are going to look ...</td>\n",
       "      <td>ye know go look</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1592344 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  target                                              tweet  \\\n",
       "0              0     1.0  @TBthatsme what!!!! Why didn't you tell us? Uh...   \n",
       "1              1     1.0  @stuartsharpe Ah you're saying being obsessed ...   \n",
       "2              2     0.0  @lejjewellery is it your birthday today? :o ye...   \n",
       "3              3     1.0  @TerrenceJ106 Hey babe follow me,love u on da ...   \n",
       "4              4     0.0  working hard as always on TNARevolution.com, t...   \n",
       "...          ...     ...                                                ...   \n",
       "1592339  1599994     0.0     @ScruffyPanther  when's the last exam mdear? X   \n",
       "1592340  1599995     0.0  Aaa lunch time. So loaded w work  but its ok. ...   \n",
       "1592341  1599996     1.0  I'm watching the movie ''The curious case of B...   \n",
       "1592342  1599997     1.0  the scene where amanda peets walks into the gl...   \n",
       "1592343  1599998     1.0  @blackquestions yes, but now i know how YOU ar...   \n",
       "\n",
       "        hashtag                                               text  \\\n",
       "0            []  what!!!! Why didn't you tell us? Uhhh... You s...   \n",
       "1            []  Ah you're saying being obsessed with his body ...   \n",
       "2            []  is it your birthday today? yeah, im worries no...   \n",
       "3            []               Hey babe follow me,love u on da show   \n",
       "4            []  working hard as always on TNARevolution.com, t...   \n",
       "...         ...                                                ...   \n",
       "1592339      []                      when's the last exam mdear? X   \n",
       "1592340      []  Aaa lunch time. So loaded w work but its ok. M...   \n",
       "1592341      []  I'm watching the movie ''The curious case of B...   \n",
       "1592342      []  the scene where amanda peets walks into the gl...   \n",
       "1592343      []  yes, but now i know how YOU are going to look ...   \n",
       "\n",
       "                                               text_helper  \n",
       "0        whi didnt tell u uhhh suck haha im kid got hom...  \n",
       "1        ah say obsess hi bodi languag onli thing keep ...  \n",
       "2        birthday today yeah im worri im go get humunga...  \n",
       "3                           hey babe follow love u da show  \n",
       "4        work hard alway tnarevolutioncom think time sl...  \n",
       "...                                                    ...  \n",
       "1592339                                  last exam mdear x  \n",
       "1592340  aaa lunch time load w work ok mani peopl arent...  \n",
       "1592341  im watch movi curiou case benjamin button good...  \n",
       "1592342  scene amanda peet walk glass door lot like lov...  \n",
       "1592343                                    ye know go look  \n",
       "\n",
       "[1592344 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5db612",
   "metadata": {},
   "source": [
    "Code fo the NN starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5645723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"preprocessed_big_cleaned_tweets_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcafde3a",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f571c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"preprocessed_big_cleaned_tweets_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6c6576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36224540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1193514 - Embedding Dim: 25 - Max Words: 50\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size, embedding_size = glove_vectors.vectors.shape\n",
    "MAX_WORDS = 50\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {} - Max Words: {}\".format(vocabulary_size, embedding_size, MAX_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "784d0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter_tokenizer(text):\n",
    "    words = text.split()\n",
    "    tokenized_word = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            tokenized_word.append(glove_vectors.key_to_index[word])\n",
    "        except KeyError:\n",
    "            tokenized_word.append(0)\n",
    "    return np.array(tokenized_word)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c30480",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized = np.array(df_train[\"text_helper\"].apply(text_splitter_tokenizer))\n",
    "X_padded = pad_sequences(X_tokenized, maxlen = MAX_WORDS, padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f47c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371bda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8b37aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 25)            29837850  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50, 25)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 50, 128)           46080     \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 80)                54080     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                5184      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 29,944,307\n",
      "Trainable params: 106,457\n",
      "Non-trainable params: 29,837,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.optimizers import adam_v2\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "model1 = Sequential([\n",
    "    Embedding(input_dim=vocabulary_size,output_dim=embedding_size,weights=[glove_vectors.vectors],input_length=MAX_WORDS,mask_zero=True,trainable=False),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(64,dropout=0.1 , return_sequences=True)),\n",
    "    Bidirectional(LSTM(40,dropout=0.4)),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(16, activation='tanh'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='tanh'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model1.summary()\n",
    "model1.compile(optimizer=adam_v2.Adam(learning_rate), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "811606db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 50, 25)            29837850  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 50, 25)            0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 512)               1101824   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 30,940,187\n",
      "Trainable params: 1,102,337\n",
      "Non-trainable params: 29,837,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    Embedding(input_dim=vocabulary_size,output_dim=embedding_size,weights=[glove_vectors.vectors],input_length=MAX_WORDS,mask_zero=True,trainable=False),\n",
    "    Dropout(0.5),\n",
    "    LSTM(512, dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.summary()\n",
    "model2.compile(optimizer=adam_v2.Adam(learning_rate), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3b3689eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 50, 25)            29837850  \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50, 25)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 50, 128)           46080     \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 80)                54080     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 29,938,091\n",
      "Trainable params: 100,241\n",
      "Non-trainable params: 29,837,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "model3 = Sequential([\n",
    "    Embedding(input_dim=vocabulary_size,output_dim=embedding_size,weights=[glove_vectors.vectors],input_length=MAX_WORDS,mask_zero=True,trainable=False),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(64,dropout=0.1 , return_sequences=True)),\n",
    "    Bidirectional(LSTM(40,dropout=0.4)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model3.summary()\n",
    "model3.compile(optimizer=adam_v2.Adam(learning_rate), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "05390a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 50, 25)            29837850  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46, 128)           16128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 29,985,691\n",
      "Trainable params: 147,841\n",
      "Non-trainable params: 29,837,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "model4=Sequential()\n",
    "model4.add(Embedding(input_dim=vocabulary_size,output_dim=embedding_size,weights=[glove_vectors.vectors],input_length=MAX_WORDS,mask_zero=True,trainable=False))\n",
    "#output(batch_size, 100, 300)\n",
    "model4.add(Conv1D(128, 5, activation='relu'))\n",
    "#output(batch_size, 96, 128), (batch_size, (size of input - size of kernel + 1) , number of filters)\n",
    "model4.add(MaxPooling1D(pool_size=1,strides=2,padding='valid'))\n",
    "#output(batch_size, 48, 128), (batch_size, (size of input - pool_size + 1)/strides, number of filters )\n",
    "model4.add(LSTM(128,dropout=0.2)) #Adding 128 lstm neurons in the layer\n",
    "#output(batch_size, 128)\n",
    "model4.add(Dense(1,activation='sigmoid'))\n",
    "model4.summary()\n",
    "model4.compile(optimizer=adam_v2.Adam(learning_rate), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c8b73098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 50, 25)            29837850  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 50, 25)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 46, 128)           16128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 23, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 80)                54080     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 30,006,955\n",
      "Trainable params: 169,105\n",
      "Non-trainable params: 29,837,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "model6 = Sequential([\n",
    "    Embedding(input_dim=vocabulary_size,output_dim=embedding_size,weights=[glove_vectors.vectors],input_length=MAX_WORDS,mask_zero=True,trainable=False),\n",
    "    Dropout(0.5),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=1,strides=2,padding='valid'),\n",
    "    Bidirectional(LSTM(64,dropout=0.1 , return_sequences=True)),\n",
    "    Bidirectional(LSTM(40,dropout=0.4)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model6.summary()\n",
    "model6.compile(optimizer=adam_v2.Adam(learning_rate), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "24635682",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 50, 25)            29837850  \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 50, 25)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 46, 128)           16128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 23, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 80)                54080     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 4)                 324       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 30,007,203\n",
      "Trainable params: 169,353\n",
      "Non-trainable params: 29,837,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "model7 = Sequential([\n",
    "    Embedding(input_dim=vocabulary_size,output_dim=embedding_size,weights=[glove_vectors.vectors],input_length=MAX_WORDS,mask_zero=True,trainable=False),\n",
    "    Dropout(0.5),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=1,strides=2,padding='valid'),\n",
    "    Bidirectional(LSTM(64,dropout=0.1 , return_sequences=True)),\n",
    "    Bidirectional(LSTM(40,dropout=0.4)),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='sigmoid'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model7.summary()\n",
    "model7.compile(optimizer=adam_v2.Adam(learning_rate), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "10fee813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "993/993 [==============================] - 177s 160ms/step - loss: 0.6784 - acc: 0.5757 - val_loss: 0.6659 - val_acc: 0.6229\n",
      "Epoch 2/5\n",
      "993/993 [==============================] - 151s 152ms/step - loss: 0.6709 - acc: 0.5965 - val_loss: 0.6568 - val_acc: 0.6175\n",
      "Epoch 3/5\n",
      "993/993 [==============================] - 157s 158ms/step - loss: 0.6658 - acc: 0.6012 - val_loss: 0.6562 - val_acc: 0.6264\n",
      "Epoch 4/5\n",
      "993/993 [==============================] - 158s 159ms/step - loss: 0.6625 - acc: 0.6081 - val_loss: 0.6605 - val_acc: 0.6161\n",
      "Epoch 5/5\n",
      "993/993 [==============================] - 152s 153ms/step - loss: 0.6623 - acc: 0.6067 - val_loss: 0.6652 - val_acc: 0.5916\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Specify training parameters: batch size and number of epochs\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Stop training if the validation loss doesn't fall for 3 consecutive epochs\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "# Train the model, reserve some training data for validation\n",
    "hist1 = model1.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_split=0.2, callbacks=[earlystopping], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bcdb10ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "993/993 [==============================] - 340s 338ms/step - loss: 0.6543 - acc: 0.6165 - val_loss: 0.6075 - val_acc: 0.6632\n",
      "Epoch 2/5\n",
      "993/993 [==============================] - 323s 325ms/step - loss: 0.6416 - acc: 0.6277 - val_loss: 0.6118 - val_acc: 0.6631\n",
      "Epoch 3/5\n",
      "993/993 [==============================] - 326s 328ms/step - loss: 0.6392 - acc: 0.6309 - val_loss: 0.6053 - val_acc: 0.6656\n",
      "Epoch 4/5\n",
      "993/993 [==============================] - 356s 358ms/step - loss: 0.6382 - acc: 0.6342 - val_loss: 0.6074 - val_acc: 0.6673\n",
      "Epoch 5/5\n",
      "993/993 [==============================] - 373s 376ms/step - loss: 0.6381 - acc: 0.6313 - val_loss: 0.6003 - val_acc: 0.6682\n"
     ]
    }
   ],
   "source": [
    "hist2 = model2.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_split=0.2, callbacks=[earlystopping], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "180dc0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "993/993 [==============================] - 213s 186ms/step - loss: 0.6388 - acc: 0.6325 - val_loss: 0.6009 - val_acc: 0.6699\n",
      "Epoch 2/5\n",
      "993/993 [==============================] - 180s 181ms/step - loss: 0.6290 - acc: 0.6444 - val_loss: 0.5988 - val_acc: 0.6809\n",
      "Epoch 3/5\n",
      "993/993 [==============================] - 179s 181ms/step - loss: 0.6237 - acc: 0.6478 - val_loss: 0.5788 - val_acc: 0.6960\n",
      "Epoch 4/5\n",
      "993/993 [==============================] - 177s 179ms/step - loss: 0.6197 - acc: 0.6536 - val_loss: 0.5757 - val_acc: 0.6915\n",
      "Epoch 5/5\n",
      "993/993 [==============================] - 177s 178ms/step - loss: 0.6174 - acc: 0.6544 - val_loss: 0.5741 - val_acc: 0.7003\n"
     ]
    }
   ],
   "source": [
    "hist3 = model3.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_split=0.2, callbacks=[earlystopping], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c9a9aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "993/993 [==============================] - 31s 28ms/step - loss: 0.6972 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.4972\n",
      "Epoch 2/5\n",
      "993/993 [==============================] - 27s 28ms/step - loss: 0.6948 - acc: 0.4999 - val_loss: 0.6948 - val_acc: 0.5028\n",
      "Epoch 3/5\n",
      "993/993 [==============================] - 27s 28ms/step - loss: 0.6947 - acc: 0.4984 - val_loss: 0.6936 - val_acc: 0.4972\n",
      "Epoch 4/5\n",
      "993/993 [==============================] - 28s 28ms/step - loss: 0.6944 - acc: 0.4991 - val_loss: 0.6970 - val_acc: 0.5028\n"
     ]
    }
   ],
   "source": [
    "hist4 = model4.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_split=0.2, callbacks=[earlystopping], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b63e503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "993/993 [==============================] - 70s 60ms/step - loss: 0.6655 - acc: 0.5996 - val_loss: 0.6429 - val_acc: 0.6440\n",
      "Epoch 2/5\n",
      "993/993 [==============================] - 58s 59ms/step - loss: 0.6543 - acc: 0.6150 - val_loss: 0.6360 - val_acc: 0.6450\n",
      "Epoch 3/5\n",
      "993/993 [==============================] - 58s 59ms/step - loss: 0.6533 - acc: 0.6163 - val_loss: 0.6360 - val_acc: 0.6468\n",
      "Epoch 4/5\n",
      "993/993 [==============================] - 58s 59ms/step - loss: 0.6530 - acc: 0.6161 - val_loss: 0.6335 - val_acc: 0.6464\n",
      "Epoch 5/5\n",
      "993/993 [==============================] - 59s 60ms/step - loss: 0.6510 - acc: 0.6189 - val_loss: 0.6300 - val_acc: 0.6516\n"
     ]
    }
   ],
   "source": [
    "hist6 = model6.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_split=0.2, callbacks=[earlystopping], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "53452f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5954/5954 [==============================] - 456s 75ms/step - loss: 0.6527 - acc: 0.6119 - val_loss: 0.6050 - val_acc: 0.6739\n",
      "Epoch 2/10\n",
      "5954/5954 [==============================] - 424s 71ms/step - loss: 0.6314 - acc: 0.6393 - val_loss: 0.5880 - val_acc: 0.6899\n",
      "Epoch 3/10\n",
      "5954/5954 [==============================] - 442s 74ms/step - loss: 0.6245 - acc: 0.6468 - val_loss: 0.5845 - val_acc: 0.6943\n",
      "Epoch 4/10\n",
      "5954/5954 [==============================] - 432s 73ms/step - loss: 0.6197 - acc: 0.6520 - val_loss: 0.5781 - val_acc: 0.6955\n",
      "Epoch 5/10\n",
      "5954/5954 [==============================] - 327s 55ms/step - loss: 0.6156 - acc: 0.6557 - val_loss: 0.5664 - val_acc: 0.7029\n",
      "Epoch 6/10\n",
      "5954/5954 [==============================] - 323s 54ms/step - loss: 0.6121 - acc: 0.6597 - val_loss: 0.5641 - val_acc: 0.7052\n",
      "Epoch 7/10\n",
      "5954/5954 [==============================] - 322s 54ms/step - loss: 0.6095 - acc: 0.6621 - val_loss: 0.5601 - val_acc: 0.7119\n",
      "Epoch 8/10\n",
      "5954/5954 [==============================] - 339s 57ms/step - loss: 0.6065 - acc: 0.6651 - val_loss: 0.5553 - val_acc: 0.7150\n",
      "Epoch 9/10\n",
      "5954/5954 [==============================] - 324s 54ms/step - loss: 0.6051 - acc: 0.6655 - val_loss: 0.5565 - val_acc: 0.7120\n",
      "Epoch 10/10\n",
      "5954/5954 [==============================] - 325s 55ms/step - loss: 0.6032 - acc: 0.6681 - val_loss: 0.5539 - val_acc: 0.7164\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Specify training parameters: batch size and number of epochs\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Stop training if the validation loss doesn't fall for 3 consecutive epochs\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "# Train the model, reserve some training data for validation\n",
    "hist1 = model7.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_split=0.2, callbacks=[earlystopping], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "88bf1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.save('./ultra_final_cnn_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3d525c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_null, X_test, y_null, y_test = train_test_split(X_test, y_test, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "808e73ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7198128700256348\n"
     ]
    }
   ],
   "source": [
    "scores = model7.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
    "print(\"Test accuracy:\", scores[1])  # scores[1] should correspond to accuracy that I passed in metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a24de",
   "metadata": {},
   "source": [
    "## Gradio Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fee2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('./ultra_final_cnn_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = new_model.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
    "print(\"Test accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5c79220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_preprocess(text):\n",
    "    \n",
    "    text = p.clean(text)\n",
    "    text = single_preprocess_data(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "#     vectorizer = CountVectorizer(max_features=vocabulary_size, preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "    # Convert the features using .toarray() for a compact representation\n",
    "    \n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def single_preprocess_data(data):\n",
    "  #Removes Numbers\n",
    "    stemmer = PorterStemmer()\n",
    "    data = data.replace('\\d+', '')\n",
    "    lower_text = data.lower()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    w_tokenizer =  TweetTokenizer()\n",
    "    \n",
    "    def stemming(text):\n",
    "        return [stemmer.stem(w) for w in text]\n",
    "    \n",
    "    def lemmatize_text(text):\n",
    "        return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "    \n",
    "    def remove_punctuation(words):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    words = lemmatize_text(lower_text)\n",
    "    words = stemming(words)\n",
    "    words = remove_punctuation(words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5d4492cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_padded_tokens(raw_input_tweet,max_len):\n",
    "    x_sequence = text_splitter_tokenizer(raw_input_tweet)\n",
    "    zeros = np.zeros(max_len - len(x_sequence))\n",
    "    return np.concatenate([x_sequence,zeros])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39e0f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter_tokenizer(text):\n",
    "    words = text.split()\n",
    "    tokenized_word = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            tokenized_word.append(glove_vectors.key_to_index[word])\n",
    "        except KeyError:\n",
    "            tokenized_word.append(0)\n",
    "    return np.array(tokenized_word)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "55d10b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000e+00 6.01762e+05 1.07855e+05 1.42930e+04 3.76915e+05 2.86000e+02\n",
      " 9.60000e+01 2.18600e+03 2.26000e+02 2.33700e+03 1.07855e+05 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00]\n"
     ]
    }
   ],
   "source": [
    "raw_input_tweet = single_preprocess(\"jnetz praising giselle on this tiktok video and one comment says “as expected, our giselle\")\n",
    "testing_single_tweet =  string_to_padded_tokens(raw_input_tweet,50)\n",
    "print(testing_single_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ec3f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroarr = np.zeros(len(testing_single_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b7b9220c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 6.01762e+05, 1.07855e+05, 1.42930e+04, 3.76915e+05,\n",
       "        2.86000e+02, 9.60000e+01, 2.18600e+03, 2.26000e+02, 2.33700e+03,\n",
       "        1.07855e+05, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_tweets = np.vstack((testing_single_tweet,zeroarr))\n",
    "\n",
    "stacked_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0079eb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.57634217],\n",
       "       [0.6497878 ]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(stacked_tweets,batch_size=2,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1a356",
   "metadata": {},
   "source": [
    "## Gradio Implemetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b3fc5909",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7864/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x208d5414ac0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def question_answer(Tweet):\n",
    "      \n",
    "    raw_input_tweet = single_preprocess(Tweet)\n",
    "\n",
    "    testing_single_tweet = string_to_padded_tokens(raw_input_tweet,50)\n",
    "    \n",
    "    zeroarr = np.zeros(len(testing_single_tweet))\n",
    "    \n",
    "    stacked_tweets = np.vstack((testing_single_tweet,zeroarr))\n",
    "\n",
    "    results = new_model.predict(stacked_tweets,batch_size=2,\n",
    "    verbose=1)\n",
    "    \n",
    "    confidence = results[0]\n",
    "    \n",
    "    if confidence < 0.4:\n",
    "        answer = \"Hate speech detected\"\n",
    "    else:\n",
    "        if confidence < 0.6:\n",
    "            answer = \"Neutral speech detected\"\n",
    "        else:\n",
    "            answer = \"Positive speech\"\n",
    "            \n",
    "    \n",
    "    \n",
    "    return (answer, confidence)\n",
    "# Return a tuple consisting of two strings: (answer, confidence) \n",
    "\n",
    "iface = gr.Interface(fn=question_answer, \n",
    "                     inputs = gr.inputs.Textbox(lines=5, placeholder=\"Insert tweet here...\"),\n",
    "                     outputs=[\"textbox\", \"text\"],theme=\"peach\",\n",
    "                     title=\"Tweet hate speech detector\",\n",
    "                     allow_flagging = \"never\",\n",
    "                    ).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
